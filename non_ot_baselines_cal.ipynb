{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, task, text_path, train_embs_path, valid_embs_path=None, pt_label=''):\n",
    "        embs, labels, unique_labels = self.load_embs_and_labels(task, text_path, train_embs_path, valid_embs_path, pt_label)\n",
    "        if task != 'pre_train':\n",
    "            print(\"{} unique labels: {}\".format(task.capitalize(), len(unique_labels)))\n",
    "        else:\n",
    "            print(\"PRE-TRAIN {} unique labels: {}\".format(pt_label, len(unique_labels)))\n",
    "        counter = collections.Counter(labels)\n",
    "        sorted_counter = dict(sorted(counter.items(), key=lambda kv: kv[1], reverse=True))\n",
    "        print(sorted_counter)\n",
    "        self.unique_labels = unique_labels\n",
    "\n",
    "        self.embs_and_labels = pd.concat([embs, labels], axis=1)\n",
    "        # 创建otdd需求的Dataset属性\n",
    "        self.classes = [str(k) for k in unique_labels] # list of unique labels string\n",
    "        self.targets = torch.tensor(list(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.embs_and_labels.iloc[idx]['pro_emb']\n",
    "        label = self.embs_and_labels.iloc[idx]['label']\n",
    "        return torch.tensor(embedding).reshape(1, -1, 1), torch.tensor(label)\n",
    "\n",
    "    def load_embs_and_labels(self, task, text_path, train_embs_path, valid_embs_path, pt_label):\n",
    "        texts = pd.read_csv(text_path)\n",
    "        if task=='pre_train':\n",
    "            texts.rename(columns={pt_label: 'label'}, inplace=True)\n",
    "        labels = texts['label']\n",
    "        unique_labels = labels.unique()\n",
    "\n",
    "        if task=='pre_train' or task=='kinase': # pre_train / kinase\n",
    "            embs = pd.read_pickle(train_embs_path)\n",
    "        else:\n",
    "            train_embs, valid_embs = pd.read_pickle(train_embs_path), pd.read_pickle(valid_embs_path)\n",
    "            embs = pd.concat([train_embs, valid_embs], axis=0)\n",
    "\n",
    "        selected_ids = texts['uniprot_id'].tolist()\n",
    "        selected_embs_flag = embs['pro_id'].map(lambda x: True if x in selected_ids else False)\n",
    "        embs = embs[selected_embs_flag].reset_index(drop=True)\n",
    "        embs = embs['pro_emb']\n",
    "\n",
    "        return embs, labels, unique_labels\n",
    "    \n",
    "def set_random_seed(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "def cal_combs_distances_mean(all_pt_tasks, t2t_distances_list):\n",
    "    mean_t2t_distances = []\n",
    "    for pt_task in all_pt_tasks:\n",
    "        spec_pt_task_distance = []\n",
    "        for t2t_distances in t2t_distances_list:\n",
    "            if pt_task in t2t_distances['pt_task'].tolist():\n",
    "                spec_pt_task_distance.append(t2t_distances[t2t_distances['pt_task']==pt_task].to_numpy()[:, :-1].reshape(1, -1))\n",
    "        spec_pt_task_distance = np.concatenate(spec_pt_task_distance, axis=0).astype(np.float32).mean(axis=0)\n",
    "        mean_t2t_distances.append(spec_pt_task_distance)\n",
    "    mean_t2t_distances = np.stack(mean_t2t_distances, axis=0)\n",
    "    return mean_t2t_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data for Pretrain-MultiTask: MLM+RMD\n",
      "PRE-TRAIN mlm unique labels: 15\n",
      "{2: 148, 10: 117, 6: 116, 8: 101, 9: 98, 4: 95, 5: 59, 14: 54, 1: 49, 3: 47, 7: 46, 12: 45, 0: 45, 11: 13, 13: 12}\n",
      "PRE-TRAIN domain unique labels: 13\n",
      "{0: 173, 1: 165, 2: 121, 3: 89, 4: 88, 5: 64, 6: 59, 7: 53, 8: 50, 9: 47, 10: 46, 11: 45, 12: 45}\n",
      "PRE-TRAIN motif unique labels: 5\n",
      "{0: 843, 1: 59, 2: 50, 3: 47, 4: 46}\n",
      "PRE-TRAIN region unique labels: 10\n",
      "{0: 202, 1: 173, 2: 165, 3: 121, 4: 89, 5: 88, 6: 64, 7: 53, 8: 45, 9: 45}\n",
      "Stability unique labels: 12\n",
      "{0: 101, 1: 87, 2: 83, 3: 57, 4: 53, 5: 34, 6: 27, 7: 20, 8: 14, 9: 11, 10: 7, 11: 6}\n",
      "Fluoresecence unique labels: 12\n",
      "{0: 142, 1: 83, 2: 80, 3: 48, 4: 38, 5: 35, 6: 20, 7: 20, 8: 14, 9: 9, 10: 6, 11: 5}\n",
      "Secondary_structrue unique labels: 15\n",
      "{9: 55, 3: 50, 1: 46, 14: 45, 8: 42, 7: 38, 13: 38, 0: 32, 6: 30, 11: 28, 2: 25, 5: 23, 12: 19, 10: 17, 4: 12}\n",
      "Pdbbind unique labels: 11\n",
      "{0: 81, 1: 70, 2: 66, 3: 58, 4: 57, 5: 56, 6: 33, 7: 28, 8: 28, 9: 18, 10: 5}\n",
      "Kinase unique labels: 2\n",
      "{0: 398, 1: 102}\n",
      "Start loading data for Pretrain-MultiTask: GO+RMD\n",
      "PRE-TRAIN domain unique labels: 13\n",
      "{0: 173, 1: 165, 2: 121, 3: 89, 4: 88, 5: 64, 6: 59, 7: 53, 8: 50, 9: 47, 10: 46, 11: 45, 12: 45}\n",
      "PRE-TRAIN motif unique labels: 5\n",
      "{0: 843, 1: 59, 2: 50, 3: 47, 4: 46}\n",
      "PRE-TRAIN region unique labels: 10\n",
      "{0: 202, 1: 173, 2: 165, 3: 121, 4: 89, 5: 88, 6: 64, 7: 53, 8: 45, 9: 45}\n",
      "PRE-TRAIN go unique labels: 9\n",
      "{0: 347, 1: 229, 2: 133, 3: 89, 4: 59, 5: 50, 6: 47, 7: 46, 8: 45}\n",
      "Stability unique labels: 12\n",
      "{0: 101, 1: 87, 2: 83, 3: 57, 4: 53, 5: 34, 6: 27, 7: 20, 8: 14, 9: 11, 10: 7, 11: 6}\n",
      "Fluoresecence unique labels: 12\n",
      "{0: 142, 1: 83, 2: 80, 3: 48, 4: 38, 5: 35, 6: 20, 7: 20, 8: 14, 9: 9, 10: 6, 11: 5}\n",
      "Secondary_structrue unique labels: 15\n",
      "{9: 55, 3: 50, 1: 46, 14: 45, 8: 42, 7: 38, 13: 38, 0: 32, 6: 30, 11: 28, 2: 25, 5: 23, 12: 19, 10: 17, 4: 12}\n",
      "Pdbbind unique labels: 11\n",
      "{0: 81, 1: 70, 2: 66, 3: 58, 4: 57, 5: 56, 6: 33, 7: 28, 8: 28, 9: 18, 10: 5}\n",
      "Kinase unique labels: 2\n",
      "{0: 398, 1: 102}\n",
      "Start loading data for Pretrain-MultiTask: MLM+GO+D\n",
      "PRE-TRAIN mlm unique labels: 15\n",
      "{2: 148, 10: 117, 6: 116, 8: 101, 9: 98, 4: 95, 5: 59, 14: 54, 1: 49, 3: 47, 7: 46, 12: 45, 0: 45, 11: 13, 13: 12}\n",
      "PRE-TRAIN domain unique labels: 13\n",
      "{0: 173, 1: 165, 2: 121, 3: 89, 4: 88, 5: 64, 6: 59, 7: 53, 8: 50, 9: 47, 10: 46, 11: 45, 12: 45}\n",
      "PRE-TRAIN go unique labels: 9\n",
      "{0: 347, 1: 229, 2: 133, 3: 89, 4: 59, 5: 50, 6: 47, 7: 46, 8: 45}\n",
      "Stability unique labels: 12\n",
      "{0: 101, 1: 87, 2: 83, 3: 57, 4: 53, 5: 34, 6: 27, 7: 20, 8: 14, 9: 11, 10: 7, 11: 6}\n",
      "Fluoresecence unique labels: 12\n",
      "{0: 142, 1: 83, 2: 80, 3: 48, 4: 38, 5: 35, 6: 20, 7: 20, 8: 14, 9: 9, 10: 6, 11: 5}\n",
      "Secondary_structrue unique labels: 15\n",
      "{9: 55, 3: 50, 1: 46, 14: 45, 8: 42, 7: 38, 13: 38, 0: 32, 6: 30, 11: 28, 2: 25, 5: 23, 12: 19, 10: 17, 4: 12}\n",
      "Pdbbind unique labels: 11\n",
      "{0: 81, 1: 70, 2: 66, 3: 58, 4: 57, 5: 56, 6: 33, 7: 28, 8: 28, 9: 18, 10: 5}\n",
      "Kinase unique labels: 2\n",
      "{0: 398, 1: 102}\n",
      "Start loading data for Pretrain-MultiTask: RMD\n",
      "PRE-TRAIN domain unique labels: 13\n",
      "{0: 173, 1: 165, 2: 121, 3: 89, 4: 88, 5: 64, 6: 59, 7: 53, 8: 50, 9: 47, 10: 46, 11: 45, 12: 45}\n",
      "PRE-TRAIN motif unique labels: 5\n",
      "{0: 843, 1: 59, 2: 50, 3: 47, 4: 46}\n",
      "PRE-TRAIN region unique labels: 10\n",
      "{0: 202, 1: 173, 2: 165, 3: 121, 4: 89, 5: 88, 6: 64, 7: 53, 8: 45, 9: 45}\n",
      "Stability unique labels: 12\n",
      "{0: 101, 1: 87, 2: 83, 3: 57, 4: 53, 5: 34, 6: 27, 7: 20, 8: 14, 9: 11, 10: 7, 11: 6}\n",
      "Fluoresecence unique labels: 12\n",
      "{0: 142, 1: 83, 2: 80, 3: 48, 4: 38, 5: 35, 6: 20, 7: 20, 8: 14, 9: 9, 10: 6, 11: 5}\n",
      "Secondary_structrue unique labels: 15\n",
      "{9: 55, 3: 50, 1: 46, 14: 45, 8: 42, 7: 38, 13: 38, 0: 32, 6: 30, 11: 28, 2: 25, 5: 23, 12: 19, 10: 17, 4: 12}\n",
      "Pdbbind unique labels: 11\n",
      "{0: 81, 1: 70, 2: 66, 3: 58, 4: 57, 5: 56, 6: 33, 7: 28, 8: 28, 9: 18, 10: 5}\n",
      "Kinase unique labels: 2\n",
      "{0: 398, 1: 102}\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "TaskCombs = ['MLM+RMD', 'GO+RMD', 'MLM+GO+D', 'RMD']\n",
    "pt_tasks_combs = [\n",
    "    ['mlm', 'domain', 'motif', 'region'],\n",
    "    ['domain', 'motif', 'region', 'go'],\n",
    "    ['mlm', 'domain', 'go'],\n",
    "    ['domain', 'motif', 'region']\n",
    "]\n",
    "# pt_tasks_combs = [list(reversed(l)) for l in pt_tasks_combs]\n",
    "all_pt_tasks = ['mlm', 'domain', 'motif', 'region', 'go']\n",
    "device = torch.device('cpu')\n",
    "p = 2\n",
    "\n",
    "# prepare data\n",
    "texts_base_dir = \"/home/brian/work/OTMTD_GH/processed_data\"\n",
    "embs_base_dir = \"/home/brian/work/OTMTD_GH/protein_embeddings_MultiTasks\"\n",
    "data_stuffs_dict = {}\n",
    "for i, comb in enumerate(TaskCombs):\n",
    "    print(\"Start loading data for Pretrain-MultiTask: {}\".format(comb))\n",
    "    # 根据multi-tasks combination构建对应的embdding hub\n",
    "    tasks_texts_embs_hub = {\n",
    "        'pre_train': [\"{}/pre_train/sampling_set.txt\".format(texts_base_dir),\n",
    "                        \"{}/pre_train_combs/pre_train_{}_pro_embs_pt.pkl\".format(embs_base_dir, comb)],\n",
    "        'stability': [\"{}/stability/sequence_go_label_{}.txt\".format(texts_base_dir, num_samples),\n",
    "                        \"{}/stability/{}->stability_pretrain_pro_embs_train.pkl\".format(embs_base_dir, comb),\n",
    "                        \"{}/stability/{}->stability_pretrain_pro_embs_valid.pkl\".format(embs_base_dir, comb)],\n",
    "        'fluoresecence': [\"{}/fluorescence/sequence_go_label_{}.txt\".format(texts_base_dir, num_samples),\n",
    "                        \"{}/fluorescence/{}->fluorescence_pretrain_pro_embs_train.pkl\".format(embs_base_dir, comb),\n",
    "                        \"{}/fluorescence/{}->fluorescence_pretrain_pro_embs_valid.pkl\".format(embs_base_dir, comb)],\n",
    "        'secondary_structrue': [\"{}/secondary_structure/sequence_go_label_{}.txt\".format(texts_base_dir, num_samples),\n",
    "                                \"{}/secondary_structure/{}->secondary_structure_pretrain_pro_embs_train.pkl\".format(embs_base_dir, comb),\n",
    "                                \"{}/secondary_structure/{}->secondary_structure_pretrain_pro_embs_valid.pkl\".format(embs_base_dir, comb)],\n",
    "        'pdbbind': [\"{}/pdbbind/sequence_go_label_{}.txt\".format(texts_base_dir, num_samples),\n",
    "                    \"{}/pdbbind/{}->pdbbind_pretrain_pro_embs_train.pkl\".format(embs_base_dir, comb),\n",
    "                    \"{}/pdbbind/{}->pdbbind_pretrain_pro_embs_dev.pkl\".format(embs_base_dir, comb)],\n",
    "        'kinase': [\"{}/kinase/sequence_go_label_{}.txt\".format(texts_base_dir, num_samples),\n",
    "                    \"{}/kinase//{}->kinase_pretrain_pro_embs_train.pkl\".format(embs_base_dir, comb)]\n",
    "    }\n",
    "\n",
    "    pt_datasets, pt_names = [], []\n",
    "    for label_task in pt_tasks_combs[i]:\n",
    "        dataset = EmbeddingDataset('pre_train', *tasks_texts_embs_hub['pre_train'], pt_label=label_task)\n",
    "        pt_datasets.append(dataset)\n",
    "        pt_names.append(label_task)\n",
    "\n",
    "    ft_datasets, ft_names = [], []\n",
    "    for task in ['stability', 'fluoresecence', 'secondary_structrue', 'pdbbind', 'kinase']: # \n",
    "        dataset = EmbeddingDataset(task, *tasks_texts_embs_hub[task])\n",
    "        ft_datasets.append(dataset)\n",
    "        ft_names.append(task.capitalize())\n",
    "\n",
    "    pt_tasks_dict = {task: idx for idx, task in enumerate(pt_tasks_combs[i])}\n",
    "\n",
    "    data_stuff = [pt_datasets, pt_names, ft_datasets, ft_names, pt_tasks_dict]\n",
    "    data_stuffs_dict[comb] = data_stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start computing for Pretrain-MultiTask: MLM+RMD\n",
      "H-score for MLM+RMD-Stability: 10.999999999999861\n",
      "H-score for MLM+RMD-Fluoresecence: 11.00000000002128\n",
      "H-score for MLM+RMD-Secondary_structrue: 13.999999999567322\n",
      "H-score for MLM+RMD-Pdbbind: 9.975969795581932\n",
      "H-score for MLM+RMD-Kinase: 0.8673495176481083\n",
      "==== x ==== ==== x ==== ==== x ==== ==== x ==== ==== x ==== \n",
      "Start computing for Pretrain-MultiTask: GO+RMD\n",
      "H-score for GO+RMD-Stability: 11.00000000000191\n",
      "H-score for GO+RMD-Fluoresecence: 10.999999999550553\n",
      "H-score for GO+RMD-Secondary_structrue: 13.99999999995598\n",
      "H-score for GO+RMD-Pdbbind: 9.97596982859892\n",
      "H-score for GO+RMD-Kinase: 0.9347650221534423\n",
      "==== x ==== ==== x ==== ==== x ==== ==== x ==== ==== x ==== \n",
      "Start computing for Pretrain-MultiTask: MLM+GO+D\n",
      "H-score for MLM+GO+D-Stability: 11.000000000001632\n",
      "H-score for MLM+GO+D-Fluoresecence: 11.000000000177018\n",
      "H-score for MLM+GO+D-Secondary_structrue: 13.999999999868336\n",
      "H-score for MLM+GO+D-Pdbbind: 9.975969858724056\n",
      "H-score for MLM+GO+D-Kinase: 0.8920242338645039\n",
      "==== x ==== ==== x ==== ==== x ==== ==== x ==== ==== x ==== \n",
      "Start computing for Pretrain-MultiTask: RMD\n",
      "H-score for RMD-Stability: 11.000000000001041\n",
      "H-score for RMD-Fluoresecence: 10.999999999991186\n",
      "H-score for RMD-Secondary_structrue: 14.000000000108052\n",
      "H-score for RMD-Pdbbind: 9.975969231639965\n",
      "H-score for RMD-Kinase: 0.998574053845914\n",
      "==== x ==== ==== x ==== ==== x ==== ==== x ==== ==== x ==== \n"
     ]
    }
   ],
   "source": [
    "def getCov(X):\n",
    "    X_mean=X-np.mean(X,axis=0,keepdims=True)\n",
    "    cov = np.divide(np.dot(X_mean.T, X_mean), len(X)-1) \n",
    "    return cov\n",
    "\n",
    "def getHscore(f,Z):\n",
    "    #Z=np.argmax(Z, axis=1)\n",
    "    Covf=getCov(f)\n",
    "    alphabetZ=list(set(Z))\n",
    "    g=np.zeros_like(f)\n",
    "    for z in alphabetZ:\n",
    "        Ef_z=np.mean(f[Z==z, :], axis=0)\n",
    "        g[Z==z]=Ef_z\n",
    "    \n",
    "    Covg=getCov(g)\n",
    "    score=np.trace(np.dot(np.linalg.pinv(Covf,rcond=1e-15), Covg))\n",
    "    return score\n",
    "\n",
    "Hscores = []\n",
    "for i, comb in enumerate(TaskCombs):\n",
    "    print(\"Start computing for Pretrain-MultiTask: {}\".format(comb))\n",
    "    data_stuff = data_stuffs_dict[comb]\n",
    "    # pt_datasets, pt_names, ft_datasets, ft_names, pt_tasks_dict = data_stuff\n",
    "    _, _, ft_datasets, ft_names, _ = data_stuff\n",
    "    comb_Hscores = []\n",
    "    # ptdataset_emb = np.stack(pt_datasets[0].embs_and_labels['pro_emb'].tolist(), axis=0)\n",
    "    for j, ft_dataset in enumerate(ft_datasets):\n",
    "        dataset_emb = np.stack(ft_dataset.embs_and_labels['pro_emb'].tolist(), axis=0)\n",
    "        dataset_label = ft_dataset.embs_and_labels['label'].values\n",
    "        score = getHscore(dataset_emb, dataset_label)\n",
    "        comb_Hscores.append(score)\n",
    "        print(\"H-score for {}-{}: {}\".format(comb, ft_names[j], score))\n",
    "    Hscores.append(comb_Hscores)\n",
    "    print('==== x ==== ' * 5)\n",
    "Hscores = np.array(Hscores)\n",
    "\n",
    "np.save(\"./results(final)/bl4_hscore.npy\", Hscores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('transferability')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Dec 11 2020, 14:32:07) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "147b18059a52f9d12650ff68dd3a2fbba920f25994ce47eb91f133cf06ed312d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
